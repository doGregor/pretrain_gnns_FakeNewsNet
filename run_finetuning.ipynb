{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8254bd46-2012-4240-8e1f-f0a0208d5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch_geometric\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.utils import compute_class_weight\n",
    "from torch_geometric.loader import DataLoader\n",
    "from predict import HGT\n",
    "from sklearn.model_selection import KFold\n",
    "from finetuning import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e99b0-1c72-4589-8613-0363465f56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_training_graphs': 50,\n",
    "    'lr_scheduler': False,\n",
    "    'hidden_channels': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f74a2d6-1859-4b75-92bd-e33c91de46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_by_path(file_name, dataset, setup):\n",
    "    path = \"../static_graphs/\" + dataset + \"/\" + setup + \"/\" + file_name + \".pickle\"\n",
    "    with open(path, 'rb') as handle:\n",
    "        return pickle.load(handle)['graph']\n",
    "\n",
    "def load_graph_ids(dataset):\n",
    "    file_path = '../' + dataset + '_train_test.pickle'\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        data = pickle.load(handle)['graph_ids']\n",
    "    return data\n",
    "\n",
    "def load_graphs_by_setup(setup, num_graphs=-1, dataset='politifact'):\n",
    "    graph_ids = load_graph_ids(dataset=dataset)\n",
    "    if num_graphs != -1:\n",
    "        graph_ids = graph_ids[:num_graphs]\n",
    "    all_graphs = []\n",
    "    for graph_id in tqdm(graph_ids):\n",
    "        graph = load_graph_by_path(file_name=graph_id, dataset=dataset, setup=setup)\n",
    "        graph['tweet'].x = graph['tweet'].x[:, :768]\n",
    "        graph['user'].x = graph['user'].x[:, :768]\n",
    "        all_graphs.append(graph)\n",
    "    return all_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7cb023-e8bd-4c10-a840-3cfa563e1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(metadata, models_folder, pretrained_name, use_pretrained=False, hidden_channels=64):\n",
    "    model_encoder = HGT(hidden_channels=hidden_channels, metadata=metadata)\n",
    "\n",
    "    if use_pretrained:\n",
    "        model_path = models_folder + '/' + pretrained_name + '.pth'\n",
    "        state = torch.load(model_path)\n",
    "        model_encoder.load_state_dict(state)\n",
    "\n",
    "    class PretrainedModel(torch.nn.Module):\n",
    "        def __init__(self, encoder, hidden_channels=hidden_channels, out_channels=2):\n",
    "            super(PretrainedModel, self).__init__()\n",
    "            # torch.manual_seed(42)\n",
    "\n",
    "            self.encoder = encoder\n",
    "            self.decoder = torch.nn.Linear(hidden_channels * 3, out_channels)\n",
    "\n",
    "        def forward(self, x_dict, edge_index_dict, batch_dict):\n",
    "            x_dict = self.encoder(x_dict, edge_index_dict, batch_dict)\n",
    "            x_dict = {key: global_mean_pool(x, batch_dict[key]) for key, x in x_dict.items()}\n",
    "            x = torch.cat([x_dict['article'], x_dict['tweet'], x_dict['user']], dim=1)\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "    model = PretrainedModel(model_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353d97b0-3e69-48a0-b1d9-0c43d556a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12214/12214 [00:39<00:00, 312.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Num train: 9771 Num test: 2443\n",
      "******************************\n",
      "Num train: 9771 Num test: 2443\n",
      "******************************\n",
      "Num train: 9771 Num test: 2443\n",
      "******************************\n",
      "Num train: 9771 Num test: 2443\n",
      "******************************\n",
      "Num train: 9772 Num test: 2442\n",
      "Epoch: 001, Loss: 0.6949, Train Acc: 0.7600, Train F1: 0.4318, Test Acc: 0.8195, Test F1: 0.4504\n",
      "Epoch: 002, Loss: 0.6944, Train Acc: 0.7600, Train F1: 0.4318, Test Acc: 0.8195, Test F1: 0.4504\n",
      "Epoch: 003, Loss: 0.6981, Train Acc: 0.7600, Train F1: 0.4318, Test Acc: 0.8195, Test F1: 0.4504\n",
      "Epoch: 004, Loss: 0.6613, Train Acc: 0.7600, Train F1: 0.4318, Test Acc: 0.8207, Test F1: 0.4661\n",
      "Epoch: 005, Loss: 0.7008, Train Acc: 0.7600, Train F1: 0.4318, Test Acc: 0.8195, Test F1: 0.4526\n",
      "Epoch: 006, Loss: 0.6853, Train Acc: 0.8200, Train F1: 0.7717, Test Acc: 0.8133, Test F1: 0.7339\n",
      "Epoch: 007, Loss: 0.6898, Train Acc: 0.8000, Train F1: 0.7520, Test Acc: 0.8060, Test F1: 0.7397\n",
      "Epoch: 008, Loss: 0.6402, Train Acc: 0.7800, Train F1: 0.5137, Test Acc: 0.8203, Test F1: 0.4638\n",
      "Epoch: 009, Loss: 0.6343, Train Acc: 0.7800, Train F1: 0.6102, Test Acc: 0.8449, Test F1: 0.6309\n",
      "Epoch: 010, Loss: 0.6277, Train Acc: 0.8000, Train F1: 0.7259, Test Acc: 0.7970, Test F1: 0.6909\n",
      "Epoch: 011, Loss: 0.5401, Train Acc: 0.8200, Train F1: 0.7814, Test Acc: 0.8097, Test F1: 0.7253\n",
      "Epoch: 012, Loss: 0.4963, Train Acc: 0.8800, Train F1: 0.8441, Test Acc: 0.8809, Test F1: 0.8085\n",
      "Epoch: 013, Loss: 0.3184, Train Acc: 0.9200, Train F1: 0.8960, Test Acc: 0.9214, Test F1: 0.8655\n",
      "Epoch: 014, Loss: 0.2127, Train Acc: 0.7800, Train F1: 0.7582, Test Acc: 0.7159, Test F1: 0.6726\n",
      "Epoch: 015, Loss: 0.4299, Train Acc: 0.9600, Train F1: 0.9480, Test Acc: 0.9194, Test F1: 0.8704\n",
      "Epoch: 016, Loss: 0.3486, Train Acc: 0.9800, Train F1: 0.9733, Test Acc: 0.9243, Test F1: 0.8705\n",
      "Epoch: 017, Loss: 0.1656, Train Acc: 0.9400, Train F1: 0.9153, Test Acc: 0.8895, Test F1: 0.7547\n",
      "Epoch: 018, Loss: 0.1180, Train Acc: 0.9800, Train F1: 0.9733, Test Acc: 0.9394, Test F1: 0.8997\n",
      "Epoch: 019, Loss: 0.1154, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.9173, Test F1: 0.8308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98537   0.87463   0.92670      2002\n",
      "           1    0.62312   0.94104   0.74977       441\n",
      "\n",
      "    accuracy                        0.88661      2443\n",
      "   macro avg    0.80425   0.90783   0.83824      2443\n",
      "weighted avg    0.91998   0.88661   0.89476      2443\n",
      "\n",
      "Epoch: 001, Loss: 0.7474, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 002, Loss: 0.5464, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 003, Loss: 0.6358, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 004, Loss: 0.6190, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 005, Loss: 0.6016, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 006, Loss: 0.6351, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 007, Loss: 0.5427, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 008, Loss: 0.5371, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 009, Loss: 0.5007, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 010, Loss: 0.5152, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 011, Loss: 0.6412, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 012, Loss: 0.5977, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8138, Test F1: 0.4487\n",
      "Epoch: 013, Loss: 0.5980, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8395, Test F1: 0.5879\n",
      "Epoch: 014, Loss: 0.5736, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8305, Test F1: 0.5452\n",
      "Epoch: 015, Loss: 0.4732, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8232, Test F1: 0.5028\n",
      "Epoch: 016, Loss: 0.3093, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8285, Test F1: 0.5324\n",
      "Epoch: 017, Loss: 0.1842, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8146, Test F1: 0.4532\n",
      "Epoch: 018, Loss: 0.0768, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8203, Test F1: 0.6110\n",
      "Epoch: 019, Loss: 0.0296, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8248, Test F1: 0.5163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.81950   0.99799   0.89998      1988\n",
      "           1    0.81818   0.03956   0.07547       455\n",
      "\n",
      "    accuracy                        0.81948      2443\n",
      "   macro avg    0.81884   0.51877   0.48772      2443\n",
      "weighted avg    0.81925   0.81948   0.74642      2443\n",
      "\n",
      "Epoch: 001, Loss: 0.8076, Train Acc: 0.0800, Train F1: 0.0741, Test Acc: 0.1764, Test F1: 0.1502\n",
      "Epoch: 002, Loss: 0.6622, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 003, Loss: 0.6975, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 004, Loss: 0.6385, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 005, Loss: 0.5468, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 006, Loss: 0.6609, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 007, Loss: 0.6546, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 008, Loss: 0.5183, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 009, Loss: 0.5734, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 010, Loss: 0.6744, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8232, Test F1: 0.4515\n",
      "Epoch: 011, Loss: 0.6039, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8244, Test F1: 0.4587\n",
      "Epoch: 012, Loss: 0.5288, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8256, Test F1: 0.4680\n",
      "Epoch: 013, Loss: 0.6083, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8240, Test F1: 0.4563\n",
      "Epoch: 014, Loss: 0.5556, Train Acc: 0.8800, Train F1: 0.5924, Test Acc: 0.8105, Test F1: 0.6268\n",
      "Epoch: 015, Loss: 0.4523, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8330, Test F1: 0.5123\n",
      "Epoch: 016, Loss: 0.3947, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8354, Test F1: 0.5322\n",
      "Epoch: 017, Loss: 0.1796, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8400, Test F1: 0.6237\n",
      "Epoch: 018, Loss: 0.0687, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8314, Test F1: 0.5258\n",
      "Epoch: 019, Loss: 0.0157, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8260, Test F1: 0.4788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.82740   0.99403   0.90309      2011\n",
      "           1    0.55556   0.03472   0.06536       432\n",
      "\n",
      "    accuracy                        0.82440      2443\n",
      "   macro avg    0.69148   0.51438   0.48423      2443\n",
      "weighted avg    0.77933   0.82440   0.75496      2443\n",
      "\n",
      "Epoch: 001, Loss: 0.7663, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 002, Loss: 0.6983, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 003, Loss: 0.6568, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 004, Loss: 0.6499, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 005, Loss: 0.6276, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 006, Loss: 0.6234, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 007, Loss: 0.6092, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 008, Loss: 0.6668, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 009, Loss: 0.5180, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 010, Loss: 0.6466, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 011, Loss: 0.5654, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 012, Loss: 0.4651, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 013, Loss: 0.4935, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8285, Test F1: 0.4531\n",
      "Epoch: 014, Loss: 0.3783, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8359, Test F1: 0.5002\n",
      "Epoch: 015, Loss: 0.3929, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8457, Test F1: 0.5701\n",
      "Epoch: 016, Loss: 0.2510, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8264, Test F1: 0.4832\n",
      "Epoch: 017, Loss: 0.1333, Train Acc: 0.8600, Train F1: 0.7255, Test Acc: 0.7225, Test F1: 0.6102\n",
      "Epoch: 018, Loss: 0.1679, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8338, Test F1: 0.5090\n",
      "Epoch: 019, Loss: 0.2133, Train Acc: 0.9800, Train F1: 0.9389, Test Acc: 0.8285, Test F1: 0.6114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86222   0.88735   0.87460      2024\n",
      "           1    0.36667   0.31504   0.33890       419\n",
      "\n",
      "    accuracy                        0.78919      2443\n",
      "   macro avg    0.61444   0.60119   0.60675      2443\n",
      "weighted avg    0.77723   0.78919   0.78272      2443\n",
      "\n",
      "Epoch: 001, Loss: 0.6177, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 002, Loss: 0.5245, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 003, Loss: 0.6630, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 004, Loss: 0.6308, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 005, Loss: 0.6264, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 006, Loss: 0.6088, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 007, Loss: 0.5303, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 008, Loss: 0.6308, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 009, Loss: 0.5991, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 010, Loss: 0.4957, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 011, Loss: 0.5159, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 012, Loss: 0.5566, Train Acc: 0.9200, Train F1: 0.4792, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 013, Loss: 0.5260, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8432, Test F1: 0.5022\n",
      "Epoch: 014, Loss: 0.4748, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8370, Test F1: 0.4606\n",
      "Epoch: 015, Loss: 0.4529, Train Acc: 0.9400, Train F1: 0.6842, Test Acc: 0.8362, Test F1: 0.4554\n",
      "Epoch: 016, Loss: 0.4056, Train Acc: 0.9600, Train F1: 0.8641, Test Acc: 0.8522, Test F1: 0.6387\n",
      "Epoch: 017, Loss: 0.2165, Train Acc: 0.9800, Train F1: 0.9232, Test Acc: 0.8403, Test F1: 0.4920\n",
      "Epoch: 018, Loss: 0.1664, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.8419, Test F1: 0.5238\n",
      "Epoch: 019, Loss: 0.0649, Train Acc: 1.0000, Train F1: 1.0000, Test Acc: 0.7699, Test F1: 0.5391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83867   0.98776   0.90713      2042\n",
      "           1    0.32432   0.03000   0.05492       400\n",
      "\n",
      "    accuracy                        0.83088      2442\n",
      "   macro avg    0.58150   0.50888   0.48102      2442\n",
      "weighted avg    0.75442   0.83088   0.76754      2442\n",
      "\n",
      "ACC [0.8866148178469095, 0.8194842406876791, 0.8243962341383545, 0.7891936144085141, 0.8308763308763308] 0.8301130475915576\n",
      "P [0.8042458609425407, 0.8188389470917352, 0.6914781089036056, 0.6144423107697231, 0.5814968814968815] 0.7021004218408973\n",
      "R [0.9078342292628007, 0.5187741835629159, 0.514377520857506, 0.6011937890893997, 0.5088785504407444] 0.6102116546426732\n",
      "F1 [0.8382371748212498, 0.4877245086165947, 0.48422706180582314, 0.6067501773014732, 0.4810241548180735] 0.5795926154726428\n"
     ]
    }
   ],
   "source": [
    "# all_graphs = load_graphs_by_setup('all_data', -1, dataset='politifact')\n",
    "all_graphs = load_graphs_by_setup('all_data', -1, dataset='gossipcop')\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(all_graphs)\n",
    "\n",
    "train_splits = []\n",
    "test_splits = []\n",
    "\n",
    "for train_index, test_index in kf.split(all_graphs):\n",
    "    print(30*\"*\")\n",
    "    X_train, X_test = itemgetter(*train_index)(all_graphs), itemgetter(*test_index)(all_graphs)\n",
    "    print(\"Num train:\", len(X_train), \"Num test:\", len(X_test))\n",
    "    train_splits.append(X_train)\n",
    "    test_splits.append(X_test)\n",
    "\n",
    "acc_all = []\n",
    "p_all = []\n",
    "r_all = []\n",
    "f1_all = []\n",
    "\n",
    "for idx, val in enumerate(train_splits):\n",
    "    if CONFIG['num_training_graphs'] is None:\n",
    "        X_train = val\n",
    "    else:\n",
    "        X_train = val[:CONFIG['num_training_graphs']]\n",
    "\n",
    "    y_tensors = []\n",
    "    for graph in X_train:\n",
    "        y_tensors.append(graph['article'].y)\n",
    "\n",
    "    class_weights = torch.tensor(compute_class_weight(class_weight='balanced', classes=np.asarray([0, 1]),\n",
    "                                                      y=torch.cat(y_tensors).cpu().detach().numpy()),\n",
    "                                 dtype=torch.float32)\n",
    "    class_weights.to(DEVICE)\n",
    "\n",
    "    train_loader = DataLoader(X_train, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(X_test, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    model = initialize_model(metadata=all_graphs[0].metadata(),\n",
    "                             use_pretrained=True,\n",
    "                             models_folder='models_separate_pureGNN', #'models_full_gos',\n",
    "                             pretrained_name='pretrained_nodes_1',\n",
    "                             hidden_channels=CONFIG['hidden_channels'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    criterion.to(DEVICE)\n",
    "    model, acc, precision, recall, f1 = train_eval_model(model=model, train_loader=train_loader, test_loader=test_loader,\n",
    "                                                         loss_fct=criterion, optimizer=optimizer, num_epochs=CONFIG['num_epochs'],\n",
    "                                                         verbose=1, use_lr_scheduler=CONFIG['lr_scheduler'])\n",
    "    acc_all.append(acc)\n",
    "    p_all.append(precision)\n",
    "    r_all.append(recall)\n",
    "    f1_all.append(f1)\n",
    "\n",
    "print(\"ACC\", acc_all, sum(acc_all) / len(acc_all))\n",
    "print(\"P\", p_all, sum(p_all) / len(p_all))\n",
    "print(\"R\", r_all, sum(r_all) / len(r_all))\n",
    "print(\"F1\", f1_all, sum(f1_all) / len(f1_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
